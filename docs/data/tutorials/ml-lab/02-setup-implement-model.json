{"kind":"project","variants":[{"traits":[{"interfaceLanguage":"swift"}],"paths":["\/tutorials\/ml-lab\/02-setup-implement-model"]}],"schemaVersion":{"patch":0,"major":0,"minor":3},"metadata":{"role":"project","categoryPathComponent":"Tutorial-Table-of-Contents","title":"Setup and implement the model","category":"ML Lab"},"identifier":{"interfaceLanguage":"swift","url":"doc:\/\/com.lambdadigamma.mllab\/tutorials\/ML-Lab\/02-setup-implement-model"},"sections":[{"estimatedTimeInMinutes":30,"chapter":"Setup and implement the classifier","title":"Setup and implement the model","content":[],"kind":"hero"},{"tasks":[{"contentSection":[{"mediaPosition":"trailing","kind":"contentAndMedia","content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"In this section we are going to load and instantiate the model."}]}]}],"title":"Setup the model","anchor":"Setup-the-model","stepsSection":[{"caption":[],"content":[{"type":"paragraph","inlineContent":[{"text":"Create a new Swift file named ","type":"text"},{"code":"ImageClassifier.swift","type":"codeVoice"},{"type":"text","text":"."},{"type":"text","text":" "},{"text":"This is going to be the main file where we implement the operation of classifing the image.","type":"text"}]}],"type":"step","code":null,"runtimePreview":null,"media":"02-01-create-class.png"},{"media":null,"caption":[],"type":"step","code":"01-import-frameworks.swift","runtimePreview":null,"content":[{"inlineContent":[{"type":"text","text":"Start by importing "},{"type":"codeVoice","code":"UIKit"},{"text":", ","type":"text"},{"type":"codeVoice","code":"Vision"},{"type":"text","text":" and "},{"type":"codeVoice","code":"CoreML"},{"type":"text","text":" at the top of the file. We need this frameworks in the course of this tutorial."}],"type":"paragraph"}]},{"media":null,"code":"02-create-class.swift","type":"step","runtimePreview":null,"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Go ahead and create a class in the Swift file."}]}],"caption":[]},{"caption":[{"type":"aside","style":"note","name":"Note","content":[{"inlineContent":[{"text":"We use a static variable to store a single instance of your model in our app. Instantiating a Core ML model is pretty resource intensive, so we want to instantiate the class once and only once.","type":"text"}],"type":"paragraph"}]}],"media":null,"runtimePreview":null,"code":"03-add-setup-method.swift","type":"step","content":[{"inlineContent":[{"type":"text","text":"We will start by loading and setting up the model in code."},{"text":" ","type":"text"},{"type":"text","text":"Add a static constant "},{"code":"model","type":"codeVoice"},{"text":" and function ","type":"text"},{"code":"setupModel","type":"codeVoice"},{"type":"text","text":" to the class. We are going to setup the model in the next steps."}],"type":"paragraph"}]},{"caption":[],"code":"04-chapter2.swift","content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"First, we have to create a "},{"type":"codeVoice","code":"MLModelConfiguration"},{"type":"text","text":"."},{"text":" ","type":"text"},{"text":"This is used to define settings for the way in which a model is executed.","type":"text"},{"type":"text","text":" "},{"text":"You can for example specify that the model should use all compute units, CPU and Neural Engine or CPU only.","type":"text"}]}],"runtimePreview":null,"type":"step","media":null},{"code":"05-chapter2.swift","runtimePreview":null,"type":"step","media":null,"content":[{"inlineContent":[{"type":"text","text":"Now, create an instance of the model wrapper class (e.g. of the class "},{"code":"SqueezeNet","type":"codeVoice"},{"text":" when using the SqueezeNet model).","type":"text"},{"type":"text","text":" "},{"text":"This class is autogenerated by Xcode after adding a ","type":"text"},{"type":"codeVoice","code":".mlmodel"},{"text":" and we are going to look at the content soon.","type":"text"}],"type":"paragraph"}],"caption":[{"inlineContent":[{"text":"We use the ","type":"text"},{"type":"codeVoice","code":"try?"},{"text":" syntax to get an ","type":"text"},{"code":"Optional<SqueezeNet>","type":"codeVoice"},{"type":"text","text":" back."},{"type":"text","text":" "},{"type":"text","text":"The variable "},{"type":"codeVoice","code":"modelWrapper"},{"text":" is ","type":"text"},{"type":"codeVoice","code":"nil"},{"text":" if the provided configuration is invalid or the model could not be found in the bundle.","type":"text"}],"type":"paragraph"}]},{"content":[],"type":"step","runtimePreview":null,"media":null,"code":null,"caption":[{"name":"Warning","content":[{"inlineContent":[{"text":"","type":"text"},{"type":"text","text":" "},{"type":"text","text":"As we don’t do any runtime changes to the model or load the model from the network, we do not need to use proper error handling here."},{"text":" ","type":"text"},{"type":"text","text":"If instantiating the model wrapper fails, we will notice during development and get a "},{"code":"fatalError","type":"codeVoice"},{"type":"text","text":" – so the App crashes."}],"type":"paragraph"}],"type":"aside","style":"warning"}]},{"caption":[{"name":"Note","content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"As a rule of thumb you can say, that all resources go into your application bundle. For example images, sound files and localization string files can be accessed via the bundle. In our case all relevant model data (our "},{"code":".mlmodel","type":"codeVoice"},{"text":" file) is being copied to the application bundle.","type":"text"}]}],"style":"note","type":"aside"}],"runtimePreview":null,"code":null,"type":"step","content":[{"inlineContent":[{"isActive":true,"type":"reference","identifier":"https:\/\/developer.apple.com\/documentation\/foundation\/bundle"},{"text":" is a class from the Foundation Framework. It is a class representing your application executable during runtime.","type":"text"},{"type":"text","text":" "},{"type":"text","text":"During compilation of your app, a bundle is being composed. The main application bundle can be accessed using "},{"type":"codeVoice","code":"Bundle.main"},{"text":".","type":"text"}],"type":"paragraph"}],"media":null},{"code":null,"content":[{"inlineContent":[{"text":"As you might know, Apple’s Platforms support a few different Machine Learning tasks like","type":"text"},{"text":" ","type":"text"},{"type":"text","text":"Image, Text, Sound, and Activity Classification"},{"type":"text","text":" "},{"type":"text","text":"just to name a few."}],"type":"paragraph"}],"runtimePreview":null,"type":"step","caption":[{"inlineContent":[{"type":"text","text":"You might have recognized that all of the given examples have different types of input data. Image Classification needs an image input, Text Classification needs a text, Sound Classification needs a sound and Activity Classification needs raw movement data in a specific format."}],"type":"paragraph"}],"media":null},{"code":null,"runtimePreview":null,"caption":[{"inlineContent":[{"text":"You might have wondered what the auto-generated model wrapper does and why it exists. So let’s take a quick view at the auto-generated code.","type":"text"},{"text":" ","type":"text"},{"type":"text","text":"You can do that by opening your "},{"type":"codeVoice","code":"SqueezeNet.mlmodel"},{"text":" in the Project Navigator and clicking on the label ","type":"text"},{"code":"SqueezeNet","type":"codeVoice"},{"type":"text","text":" right next to the "},{"code":"Model Class","type":"codeVoice"},{"type":"text","text":" label."}],"type":"paragraph"},{"style":"warning","name":"Warning","type":"aside","content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Make sure that you compiled your app as the code of the class is generated during compilation. You may have to comment out the existing code in order to compile successfully."}]}]}],"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Everything we did by now, is loading and instantiating our model wrapper from the "},{"type":"codeVoice","code":"Bundle"},{"type":"text","text":"."}]}],"media":"02-02-click-model-class.png","type":"step"},{"caption":[],"media":null,"runtimePreview":null,"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"You will be greeted by a wall of text. But no worries, we just need to take a quick look at it, as the "},{"type":"codeVoice","code":"Vision"},{"type":"text","text":" framework will do the heavy lifting for us."}]}],"type":"step","code":null},{"runtimePreview":null,"type":"step","code":"01-SqueezeNet.swift","caption":[],"media":null,"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"This file should contain three classes."},{"text":" ","type":"text"},{"type":"text","text":"Depending on the name of your model it has a class for input, output and for loading and prediction."}]}]},{"content":[{"inlineContent":[{"text":"The ","type":"text"},{"code":"SqueezeNetInput","type":"codeVoice"},{"type":"text","text":" class has an important variable "},{"type":"codeVoice","code":"image"},{"text":" which is the input for the model.","type":"text"},{"type":"text","text":" "},{"type":"codeVoice","code":"CVPixelBuffer"},{"type":"text","text":" is a very low-level class from "},{"code":"CoreVideo","type":"codeVoice"},{"text":" and stores images in a buffer.","type":"text"}],"type":"paragraph"}],"media":null,"runtimePreview":null,"type":"step","caption":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Because working with them is not that straight forward, we get a few so called "},{"type":"codeVoice","code":"convenience"},{"type":"text","text":" initializer by default. As the name implies a "},{"code":"convenience init","type":"codeVoice"},{"type":"text","text":" should simplify instantiating an object of this class."},{"type":"text","text":" "},{"text":"For example we get options to create a model input from ","type":"text"},{"type":"codeVoice","code":"UIImage"},{"type":"text","text":" and "},{"type":"codeVoice","code":"URL"},{"text":".","type":"text"}]}],"code":"SqueezeNet-02.swift"},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"The output class provides a dictionary which maps class labels to their calculated likelihood."}]}],"code":"SqueezeNet-03.swift","caption":[],"media":null,"type":"step","runtimePreview":null},{"caption":[{"type":"paragraph","inlineContent":[{"type":"text","text":"You may have noticed that the input of the model takes only input buffers of size 227x227 pixels."}]}],"content":[{"type":"paragraph","inlineContent":[{"text":"The ","type":"text"},{"type":"codeVoice","code":"SqueezeNet"},{"text":" class has exactly the initializer, which we used in the ","type":"text"},{"type":"codeVoice","code":"ImageClassifier"},{"text":" class.","type":"text"},{"text":" ","type":"text"},{"type":"text","text":"It also has methods to predict an output from a given output."}]}],"code":"SqueezeNet-04.swift","type":"step","runtimePreview":null,"media":null},{"media":null,"content":[{"inlineContent":[{"text":"But how do we convert our images to the required format?","type":"text"},{"type":"text","text":" "},{"type":"text","text":"Thats where the Vision Framework comes into play."},{"type":"text","text":" "},{"text":"The Vision Framework provides features to execute common computer vision problems like text recognition on still images and videos.","type":"text"},{"type":"text","text":" "},{"text":"It also simplifies dealing with Core ML models so we are going to setup a ","type":"text"},{"code":"VNCoreMLModel","type":"codeVoice"},{"text":" next.","type":"text"}],"type":"paragraph"}],"caption":[],"runtimePreview":null,"type":"step","code":null},{"type":"step","code":"06-chapter2.swift","runtimePreview":null,"caption":[],"content":[{"inlineContent":[{"type":"text","text":"Go ahead and create a "},{"type":"codeVoice","code":"VNCoreMLModel"},{"type":"text","text":" from the underlying "},{"type":"codeVoice","code":"MLModel"},{"type":"text","text":" of the "},{"type":"codeVoice","code":"SqueezeNet"},{"type":"text","text":" wrapper instance."}],"type":"paragraph"}],"media":null},{"caption":[],"code":null,"content":[{"inlineContent":[{"type":"text","text":"We now have successfully setup our model and will look at how to use this "},{"code":"VNCoreMLModel","type":"codeVoice"},{"text":" in the next section.","type":"text"}],"type":"paragraph"}],"type":"step","runtimePreview":null,"media":null}]},{"contentSection":[{"kind":"contentAndMedia","content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Setup the additional models we need for the "},{"type":"codeVoice","code":"ImageClassifier"},{"type":"text","text":"."}]}],"mediaPosition":"trailing"}],"stepsSection":[{"runtimePreview":null,"code":"Section2-01.swift","media":null,"type":"step","caption":[],"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Create a new Swift file "},{"type":"codeVoice","code":"ImageClassificationPredication.swift"},{"type":"text","text":" and add a struct."},{"text":" ","type":"text"},{"type":"text","text":"This will store a single result of your classification operation. E.g. label of “elephant” and confidence "},{"type":"codeVoice","code":"0.94"},{"text":".","type":"text"}]}]},{"code":"07-chapter2.swift","media":null,"runtimePreview":null,"caption":[{"type":"paragraph","inlineContent":[{"text":"A typealias works like a shorthand. Everywhere you write ","type":"text"},{"code":"ImageClassificationHandler","type":"codeVoice"},{"text":", it is the same like replacing it with the right side.","type":"text"}]}],"content":[{"inlineContent":[{"type":"text","text":"Add a typealias "},{"code":"ImageClassificationHandler","type":"codeVoice"},{"type":"text","text":" to the top of the file "},{"code":"ImageClassifier.swift","type":"codeVoice"},{"text":".","type":"text"},{"text":" ","type":"text"},{"type":"text","text":"This type will be used as a shorthand for the result closure. It returns an array of "},{"type":"codeVoice","code":"ImageClassificationPredication"},{"text":" and does not exect any return types.","type":"text"}],"type":"paragraph"}],"type":"step"},{"caption":[{"type":"paragraph","inlineContent":[{"type":"text","text":"A handler is a closure being passed from the caller to react to the results."}]}],"runtimePreview":null,"code":null,"type":"step","media":null,"content":[{"inlineContent":[{"text":"A ","type":"text"},{"code":"VNRequest","type":"codeVoice"},{"type":"text","text":" is used to execute an image classification operation."},{"text":" ","type":"text"},{"text":"Each request get’s an image input and has a handler which is called with the image classification predictions calculated with the model.","type":"text"}],"type":"paragraph"}]},{"media":null,"type":"step","caption":[],"code":"08-chapter2.swift","runtimePreview":null,"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"We store a reference of each request feed into the "},{"type":"codeVoice","code":"ImageClassifier"},{"text":" along with the provided handler.","type":"text"}]}]},{"caption":[{"inlineContent":[{"text":"We create an initializer which maps the orientation of a ","type":"text"},{"type":"codeVoice","code":"UIImage"},{"type":"text","text":" (which we can easily handle) to a "},{"code":"CGImagePropertyOrientation","type":"codeVoice"},{"text":".","type":"text"}],"type":"paragraph"}],"type":"step","content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Create a new file and add an extension to the "},{"type":"codeVoice","code":"CGImagePropertyOrientation"},{"text":".","type":"text"},{"type":"text","text":" "},{"text":"When feeding an image into the Vision framework we need to provide the image orientiation as a ","type":"text"},{"type":"codeVoice","code":"CoreGraphics"},{"type":"text","text":" image orientation."}]}],"media":null,"code":"CGImagePropertyOrientation+Extensions.swift","runtimePreview":null},{"code":"09-chapter2.swift","type":"step","media":null,"caption":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Next, create an image classification request. We are going to implement the function "},{"type":"codeVoice","code":"createImageClassificationRequest()"},{"type":"text","text":" in a further step."}]}],"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Add a "},{"code":"makePredictions","type":"codeVoice"},{"type":"text","text":" function."},{"type":"text","text":" "},{"type":"text","text":"First, obtain the "},{"type":"codeVoice","code":"CGImagePropertyOrientation"},{"type":"text","text":" and the "},{"type":"codeVoice","code":"CGImage"},{"text":" from the provided ","type":"text"},{"code":"UIImage","type":"codeVoice"},{"type":"text","text":"."}]}],"runtimePreview":null},{"code":"10-chapter2.swift","runtimePreview":null,"caption":[{"type":"paragraph","inlineContent":[{"text":"So create the ","type":"text"},{"type":"codeVoice","code":"VNImageRequestHandler"},{"text":" with a given image and the previously obtained orientation. After that, call the throwing function ","type":"text"},{"type":"codeVoice","code":"perform"},{"type":"text","text":" on the handler which takes an array of "},{"type":"codeVoice","code":"VNRequest"},{"text":" as parameters.","type":"text"}]}],"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Store the request in the "},{"code":"predictionHandlers","type":"codeVoice"},{"text":" array with the request as the key and the completion handler passed to the function as the value. This is going to be called when the following request handler finished the classification job.","type":"text"}]}],"media":null,"type":"step"},{"caption":[{"type":"paragraph","inlineContent":[{"text":"Add this method to create a new ","type":"text"},{"code":"VNCoreMLRequest","type":"codeVoice"},{"text":". It uses the staticly available model which we setup earlier. We use a image crop and scale options of ","type":"text"},{"type":"codeVoice","code":".centerCrop"},{"text":" – there may be cases where you want a different behaviour, but for our use case this is good.","type":"text"}]},{"name":"Warning","type":"aside","content":[{"inlineContent":[{"type":"text","text":"Again, you see a warning that "},{"code":"visionRequestHandler","type":"codeVoice"},{"text":" is not available. We will implement this in the next step.","type":"text"}],"type":"paragraph"}],"style":"warning"}],"media":null,"runtimePreview":null,"content":[{"inlineContent":[{"text":"You can see, that there is an error ","type":"text"},{"code":"Cannot find 'createImageClassificationRequest' in scope","type":"codeVoice"},{"type":"text","text":". We are going to fix that now."}],"type":"paragraph"}],"code":"11-chapter2.swift","type":"step"},{"code":"12-chapter2.swift","type":"step","media":null,"runtimePreview":null,"caption":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Based on the request which get’s completed, we get the handler function from the "},{"type":"codeVoice","code":"predictionHandlers"},{"type":"text","text":" dictionary. Next, we need an optional predictions array in which we will store the results produced by the model."}]}],"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"As the final step, we implement the "},{"type":"codeVoice","code":"visionRequestHandler"},{"type":"text","text":"."},{"type":"text","text":" "},{"type":"text","text":"This will be the function which get’s called every time a "},{"code":"VNCoreMLRequest","type":"codeVoice"},{"text":" succeeds or fails and handles calling the right completion functions.","type":"text"}]}]},{"type":"step","caption":[{"inlineContent":[{"type":"text","text":"We also check if the request failed with an error or has no results."}],"type":"paragraph"}],"media":null,"code":"13-chapter2.swift","runtimePreview":null,"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Next, we use a defer callback to call the retrieved predicition handler with the obtained results."}]}]},{"caption":[{"type":"aside","content":[{"type":"paragraph","inlineContent":[{"text":"Image classifiers only produce classification observations. However, other Core ML model types can produce other observations. For example, a style transfer model produces ","type":"text"},{"code":"VNPixelBufferObservation","type":"codeVoice"},{"text":" instances.","type":"text"}]}],"style":"note","name":"Note"}],"type":"step","runtimePreview":null,"code":"14-chapter2.swift","content":[{"type":"paragraph","inlineContent":[{"text":"We now cast the request’s results as an ","type":"text"},{"code":"VNClassificationObservation","type":"codeVoice"},{"text":" array.","type":"text"}]}],"media":null}],"anchor":"Implement-the-classification","title":"Implement the classification"},{"contentSection":[{"content":[{"inlineContent":[{"text":"Let’s look at what we build. You may struggle to understand how data flows here, so we will do a short recap.","type":"text"}],"type":"paragraph"}],"mediaPosition":"trailing","kind":"contentAndMedia"}],"stepsSection":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"We setup a "},{"code":"VNCoreMLModel","type":"codeVoice"},{"type":"text","text":" with our image classifier model "},{"code":"SqueezeNet","type":"codeVoice"},{"type":"text","text":"."}]}],"type":"step","caption":[],"media":null,"code":null,"runtimePreview":null},{"code":"01-Example.swift","type":"step","caption":[],"media":null,"runtimePreview":null,"content":[{"inlineContent":[{"text":"We as a developer can now create an instance of the ","type":"text"},{"type":"codeVoice","code":"ImageClassifier"},{"text":" class and call ","type":"text"},{"code":"makePredictions","type":"codeVoice"},{"type":"text","text":" with an image."},{"text":" ","type":"text"},{"type":"text","text":"The results are available in the trailing closure."}],"type":"paragraph"}]}],"anchor":"How-to-use-the-image-classifier","title":"How to use the image classifier"}],"kind":"tasks"}],"hierarchy":{"paths":[["doc:\/\/com.lambdadigamma.mllab\/tutorials\/Tutorial-Table-of-Contents","doc:\/\/com.lambdadigamma.mllab\/tutorials\/Tutorial-Table-of-Contents\/$volume","doc:\/\/com.lambdadigamma.mllab\/tutorials\/Tutorial-Table-of-Contents\/Setup-and-implement-the-classifier"]],"modules":[{"projects":[{"reference":"doc:\/\/com.lambdadigamma.mllab\/tutorials\/ML-Lab\/01-Setup-the-project","sections":[{"kind":"task","reference":"doc:\/\/com.lambdadigamma.mllab\/tutorials\/ML-Lab\/01-Setup-the-project#Getting-a-model"},{"reference":"doc:\/\/com.lambdadigamma.mllab\/tutorials\/ML-Lab\/01-Setup-the-project#Adding-your-model-to-Xcode","kind":"task"},{"reference":"doc:\/\/com.lambdadigamma.mllab\/tutorials\/ML-Lab\/01-Setup-the-project#Exploring-the-model-in-Xcode","kind":"task"},{"reference":"doc:\/\/com.lambdadigamma.mllab\/tutorials\/ML-Lab\/01-Setup-the-project#Summary","kind":"task"}]}],"reference":"doc:\/\/com.lambdadigamma.mllab\/tutorials\/Tutorial-Table-of-Contents\/Add-the-model-to-your-project"},{"reference":"doc:\/\/com.lambdadigamma.mllab\/tutorials\/Tutorial-Table-of-Contents\/Setup-and-implement-the-classifier","projects":[{"reference":"doc:\/\/com.lambdadigamma.mllab\/tutorials\/ML-Lab\/02-setup-implement-model","sections":[{"kind":"task","reference":"doc:\/\/com.lambdadigamma.mllab\/tutorials\/ML-Lab\/02-setup-implement-model#Setup-the-model"},{"reference":"doc:\/\/com.lambdadigamma.mllab\/tutorials\/ML-Lab\/02-setup-implement-model#Implement-the-classification","kind":"task"},{"kind":"task","reference":"doc:\/\/com.lambdadigamma.mllab\/tutorials\/ML-Lab\/02-setup-implement-model#How-to-use-the-image-classifier"}]}]},{"projects":[],"reference":"doc:\/\/com.lambdadigamma.mllab\/tutorials\/Tutorial-Table-of-Contents\/Add-the-next-chapter-title-here."}],"reference":"doc:\/\/com.lambdadigamma.mllab\/tutorials\/Tutorial-Table-of-Contents"},"references":{"11-chapter2.swift":{"type":"file","syntax":"swift","content":["import UIKit","import Vision","import CoreML","","typealias ImageClassificationHandler = (_ predictions: [ImageClassificationPredication]?) -> Void","","class ImageClassifier {","    ","    static let model = setupModel()","    ","    static func setupModel() -> VNCoreMLModel {}","    ","    \/\/ MARK: - Request Handling -","    ","    private var predictionHandlers = [VNRequest: ImageClassificationHandler]()","    ","    func makePredictions(","        for photo: UIImage,","        completionHandler: @escaping ImageClassificationHandler","    ) throws {","        ","        let orientation = CGImagePropertyOrientation(photo.imageOrientation)","        ","        guard let photoImage = photo.cgImage else {","            fatalError(\"Photo doesn't have underlying CGImage.\")","        }","        ","        let imageClassificationRequest = createImageClassificationRequest()","        ","        predictionHandlers[imageClassificationRequest] = completionHandler","        ","        let handler = VNImageRequestHandler(","            cgImage: photoImage,","            orientation: orientation","        )","        ","        try handler.perform([imageClassificationRequest])","        ","    }","    ","    private func createImageClassificationRequest() -> VNCoreMLRequest {","        ","        let imageClassificationRequest = VNCoreMLRequest(","            model: Self.model,","            completionHandler: visionRequestHandler","        )","        ","        imageClassificationRequest.imageCropAndScaleOption = .centerCrop","        ","        return imageClassificationRequest","    }","    ","}"],"fileName":"ImageClassifier.swift","fileType":"swift","highlights":[{"line":41},{"line":42},{"line":43},{"line":44},{"line":45},{"line":46},{"line":47},{"line":48},{"line":49},{"line":50},{"line":51},{"line":52}],"identifier":"11-chapter2.swift"},"Section2-01.swift":{"identifier":"Section2-01.swift","type":"file","fileType":"swift","content":["import Foundation","import Vision","","struct ImageClassificationPredication {","    ","    let label: String","    ","    let confidence: VNConfidence","    ","}"],"highlights":[],"syntax":"swift","fileName":"ImageClassificationPredication.swift"},"CGImagePropertyOrientation+Extensions.swift":{"type":"file","syntax":"swift","content":["import UIKit","import ImageIO","","extension CGImagePropertyOrientation {","    ","    init(_ orientation: UIImage.Orientation) {","        switch orientation {","            case .up: self = .up","            case .down: self = .down","            case .left: self = .left","            case .right: self = .right","            case .upMirrored: self = .upMirrored","            case .downMirrored: self = .downMirrored","            case .leftMirrored: self = .leftMirrored","            case .rightMirrored: self = .rightMirrored","            @unknown default: self = .up","        }","    }","    ","}"],"fileName":"CGImagePropertyOrientation+Extensions.swift","fileType":"swift","highlights":[],"identifier":"CGImagePropertyOrientation+Extensions.swift"},"doc://com.lambdadigamma.mllab/tutorials/Tutorial-Table-of-Contents/Setup-and-implement-the-classifier":{"type":"topic","title":"Setup and implement the classifier","kind":"article","url":"\/tutorials\/tutorial-table-of-contents\/setup-and-implement-the-classifier","role":"article","abstract":[],"identifier":"doc:\/\/com.lambdadigamma.mllab\/tutorials\/Tutorial-Table-of-Contents\/Setup-and-implement-the-classifier"},"01-SqueezeNet.swift":{"type":"file","syntax":"swift","content":["import CoreML","","","\/\/\/ Model Prediction Input Type","class SqueezeNetInput : MLFeatureProvider {","    ","    \/\/ ...","    ","}","","\/\/\/ Model Prediction Output Type","class SqueezeNetOutput : MLFeatureProvider {","    ","    \/\/ ...","    ","}","","\/\/\/ Class for model loading and prediction","class SqueezeNet {","    ","    \/\/ ...","    ","}"],"fileName":"SqueezeNet.swift","fileType":"swift","highlights":[],"identifier":"01-SqueezeNet.swift"},"02-02-click-model-class.png":{"identifier":"02-02-click-model-class.png","type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/02-02-click-model-class.png"}],"alt":""},"doc://com.lambdadigamma.mllab/tutorials/ML-Lab/01-Setup-the-project":{"title":"Add the model to your project","type":"topic","kind":"project","url":"\/tutorials\/ml-lab\/01-setup-the-project","role":"project","abstract":[{"text":"Learn how to add an Core ML file to your app and explore the built-in capabilities of Xcode.","type":"text"}],"estimatedTime":"10min","identifier":"doc:\/\/com.lambdadigamma.mllab\/tutorials\/ML-Lab\/01-Setup-the-project"},"01-Example.swift":{"type":"file","syntax":"swift","fileName":"Example.swift","content":["let imageClassifier = imageClassifier()","","\/\/ Load an image from Asset Catalogue","let image = UIImage(named: \"example\")","","do {","    ","    \/\/ Make a prediction and print the results.","    try imageClassifier.makePredictions(","        for: image","    ) { predictions in","        ","        if let predictions {","            ","            for prediction in predictions {","                print(\"\\(prediction.classification): \\(prediction.confidence)\")","            }","            ","        } else {","            print(\"No result from image classifier.\")","        }","        ","    }","    ","} catch {","    print(\"Vision was unable to make a prediction...\\n\\n\\(error.localizedDescription)\")","}"],"fileType":"swift","highlights":[],"identifier":"01-Example.swift"},"02-01-create-class.png":{"type":"image","identifier":"02-01-create-class.png","variants":[{"traits":["1x","light"],"url":"\/images\/02-01-create-class.png"}],"alt":""},"doc://com.lambdadigamma.mllab/tutorials/ML-Lab/01-Setup-the-project#Adding-your-model-to-Xcode":{"type":"section","title":"Adding your model to Xcode","kind":"section","url":"\/tutorials\/ml-lab\/01-setup-the-project#Adding-your-model-to-Xcode","role":"pseudoSymbol","abstract":[{"text":"Learn how to add an Core ML file to your app and explore the built-in capabilities of Xcode.","type":"text"}],"identifier":"doc:\/\/com.lambdadigamma.mllab\/tutorials\/ML-Lab\/01-Setup-the-project#Adding-your-model-to-Xcode"},"doc://com.lambdadigamma.mllab/tutorials/Tutorial-Table-of-Contents":{"title":"Implementing an image classifier with Core ML","type":"topic","kind":"overview","url":"\/tutorials\/tutorial-table-of-contents","role":"overview","abstract":[{"text":"Add and implement an image classifier Core ML model into your application.","type":"text"}],"identifier":"doc:\/\/com.lambdadigamma.mllab\/tutorials\/Tutorial-Table-of-Contents"},"SqueezeNet-02.swift":{"type":"file","syntax":"swift","fileName":"SqueezeNet.swift","content":["import CoreML","","","\/\/\/ Model Prediction Input Type","class SqueezeNetInput : MLFeatureProvider {","    ","    \/\/\/ Input image to be classified as color (kCVPixelFormatType_32BGRA) image buffer, 227 pixels wide by 227 pixels high","    var image: CVPixelBuffer","    ","    \/\/ ...","    ","    init(image: CVPixelBuffer) {","        self.image = image","    }","    ","    convenience init(imageWith image: CGImage) throws {","        \/\/ ...","    }","    ","    convenience init(imageAt image: URL) throws {","        \/\/ ...","    }","    ","    func setImage(with image: CGImage) throws {","        \/\/ ...","    }","    ","    func setImage(with image: URL) throws {","        \/\/ ...","    }","    ","}","","\/\/\/ Model Prediction Output Type","class SqueezeNetOutput : MLFeatureProvider {","    ","    \/\/ ...","    ","}","","\/\/\/ Class for model loading and prediction","class SqueezeNet {","    ","    \/\/ ...","    ","}"],"fileType":"swift","highlights":[{"line":7},{"line":8},{"line":9},{"line":12},{"line":13},{"line":14},{"line":15},{"line":16},{"line":17},{"line":18},{"line":19},{"line":20},{"line":21},{"line":22},{"line":23},{"line":24},{"line":25},{"line":26},{"line":27},{"line":28},{"line":29},{"line":30},{"line":31}],"identifier":"SqueezeNet-02.swift"},"https://developer.apple.com/documentation/foundation/bundle":{"titleInlineContent":[{"text":"Bundle","type":"text"}],"url":"https:\/\/developer.apple.com\/documentation\/foundation\/bundle","identifier":"https:\/\/developer.apple.com\/documentation\/foundation\/bundle","title":"Bundle","type":"link"},"10-chapter2.swift":{"type":"file","syntax":"swift","fileName":"ImageClassifier.swift","content":["import UIKit","import Vision","import CoreML","","typealias ImageClassificationHandler = (_ predictions: [ImageClassificationPredication]?) -> Void","","class ImageClassifier {","    ","    static let model = setupModel()","    ","    static func setupModel() -> VNCoreMLModel {}","    ","    \/\/ MARK: - Request Handling -","    ","    private var predictionHandlers = [VNRequest: ImageClassificationHandler]()","    ","    func makePredictions(","        for photo: UIImage,","        completionHandler: @escaping ImageClassificationHandler","    ) throws {","        ","        let orientation = CGImagePropertyOrientation(photo.imageOrientation)","        ","        guard let photoImage = photo.cgImage else {","            fatalError(\"Photo doesn't have underlying CGImage.\")","        }","        ","        let imageClassificationRequest = createImageClassificationRequest()","        ","        predictionHandlers[imageClassificationRequest] = completionHandler","        ","        let handler = VNImageRequestHandler(","            cgImage: photoImage,","            orientation: orientation","        )","        ","        try handler.perform([imageClassificationRequest])","        ","    }","    ","}"],"fileType":"swift","highlights":[{"line":30},{"line":31},{"line":32},{"line":33},{"line":34},{"line":35},{"line":36},{"line":37},{"line":38}],"identifier":"10-chapter2.swift"},"09-chapter2.swift":{"type":"file","syntax":"swift","content":["import UIKit","import Vision","import CoreML","","typealias ImageClassificationHandler = (_ predictions: [ImageClassificationPredication]?) -> Void","","class ImageClassifier {","    ","    static let model = setupModel()","    ","    static func setupModel() -> VNCoreMLModel {}","    ","    \/\/ MARK: - Request Handling -","    ","    private var predictionHandlers = [VNRequest: ImageClassificationHandler]()","    ","    func makePredictions(","        for photo: UIImage,","        completionHandler: @escaping ImageClassificationHandler","    ) throws {","        ","        let orientation = CGImagePropertyOrientation(photo.imageOrientation)","        ","        guard let photoImage = photo.cgImage else {","            fatalError(\"Photo doesn't have underlying CGImage.\")","        }","        ","        let imageClassificationRequest = createImageClassificationRequest()","        ","    }","    ","}"],"fileName":"ImageClassifier.swift","fileType":"swift","highlights":[],"identifier":"09-chapter2.swift"},"doc://com.lambdadigamma.mllab/tutorials/ML-Lab/01-Setup-the-project#Exploring-the-model-in-Xcode":{"type":"section","title":"Exploring the model in Xcode","kind":"section","url":"\/tutorials\/ml-lab\/01-setup-the-project#Exploring-the-model-in-Xcode","role":"pseudoSymbol","abstract":[{"text":"Learn how to add an Core ML file to your app and explore the built-in capabilities of Xcode.","type":"text"}],"identifier":"doc:\/\/com.lambdadigamma.mllab\/tutorials\/ML-Lab\/01-Setup-the-project#Exploring-the-model-in-Xcode"},"doc://com.lambdadigamma.mllab/tutorials/ML-Lab/02-setup-implement-model#Setup-the-model":{"title":"Setup the model","type":"section","kind":"section","url":"\/tutorials\/ml-lab\/02-setup-implement-model#Setup-the-model","role":"pseudoSymbol","abstract":[],"identifier":"doc:\/\/com.lambdadigamma.mllab\/tutorials\/ML-Lab\/02-setup-implement-model#Setup-the-model"},"doc://com.lambdadigamma.mllab/tutorials/ML-Lab/02-setup-implement-model#How-to-use-the-image-classifier":{"type":"section","title":"How to use the image classifier","kind":"section","url":"\/tutorials\/ml-lab\/02-setup-implement-model#How-to-use-the-image-classifier","role":"pseudoSymbol","abstract":[],"identifier":"doc:\/\/com.lambdadigamma.mllab\/tutorials\/ML-Lab\/02-setup-implement-model#How-to-use-the-image-classifier"},"doc://com.lambdadigamma.mllab/tutorials/ML-Lab/02-setup-implement-model":{"title":"Setup and implement the model","type":"topic","kind":"project","url":"\/tutorials\/ml-lab\/02-setup-implement-model","role":"project","abstract":[],"estimatedTime":"30min","identifier":"doc:\/\/com.lambdadigamma.mllab\/tutorials\/ML-Lab\/02-setup-implement-model"},"14-chapter2.swift":{"type":"file","syntax":"swift","fileName":"ImageClassifier.swift","content":["import UIKit","import Vision","import CoreML","","typealias ImageClassificationHandler = (_ predictions: [ImageClassificationPredication]?) -> Void","","class ImageClassifier {","    ","    static let model = setupModel()","    ","    static func setupModel() -> VNCoreMLModel {}","    ","    \/\/ MARK: - Request Handling -","    ","    private var predictionHandlers = [VNRequest: ImageClassificationHandler]()","    ","    func makePredictions(","        for photo: UIImage,","        completionHandler: @escaping ImageClassificationHandler","    ) throws {","        ","        let orientation = CGImagePropertyOrientation(photo.imageOrientation)","        ","        guard let photoImage = photo.cgImage else {","            fatalError(\"Photo doesn't have underlying CGImage.\")","        }","        ","        let imageClassificationRequest = createImageClassificationRequest()","        ","        predictionHandlers[imageClassificationRequest] = completionHandler","        ","        let handler = VNImageRequestHandler(","            cgImage: photoImage,","            orientation: orientation","        )","        ","        try handler.perform([imageClassificationRequest])","        ","    }","    ","    private func createImageClassificationRequest() -> VNCoreMLRequest {","        ","        let imageClassificationRequest = VNCoreMLRequest(","            model: Self.model,","            completionHandler: visionRequestHandler","        )","        ","        imageClassificationRequest.imageCropAndScaleOption = .centerCrop","        ","        return imageClassificationRequest","    }","    ","    private func visionRequestHandler(_ request: VNRequest, error: Error?) {","        ","        guard let predictionHandler = predictionHandlers.removeValue(","            forKey: request","        ) else {","            fatalError(\"Every request must have a prediction handler.\")","        }","        ","        var predictions: [ImageClassificationPredication]? = nil","        ","        \/\/ Call the client's completion handler after the method returns.","        defer {","            \/\/ Send the predictions back to the client.","            predictionHandler(predictions)","        }","        ","        if let error = error {","            print(\"Vision image classification error: \\(error.localizedDescription)\")","            return","        }","        ","        if request.results == nil {","            print(\"Vision request had no results.\")","            return","        }","        ","        guard let observations = request.results as? [VNClassificationObservation] else {","            print(\"VNRequest produced the wrong result type: \\(type(of: request.results)).\")","            return","        }","        ","        predictions = observations.map { observation in","            ImageClassificationPredication(","                label: observation.identifier,","                confidence: observation.confidence","            )","        }","        ","    }","    ","}"],"fileType":"swift","highlights":[{"line":79},{"line":80},{"line":81},{"line":82},{"line":84},{"line":85},{"line":86},{"line":87},{"line":88},{"line":89},{"line":90}],"identifier":"14-chapter2.swift"},"doc://com.lambdadigamma.mllab/tutorials/Tutorial-Table-of-Contents/Add-the-model-to-your-project":{"url":"\/tutorials\/tutorial-table-of-contents\/add-the-model-to-your-project","role":"article","type":"topic","abstract":[],"title":"Add the model to your project","kind":"article","identifier":"doc:\/\/com.lambdadigamma.mllab\/tutorials\/Tutorial-Table-of-Contents\/Add-the-model-to-your-project"},"doc://com.lambdadigamma.mllab/tutorials/ML-Lab/02-setup-implement-model#Implement-the-classification":{"url":"\/tutorials\/ml-lab\/02-setup-implement-model#Implement-the-classification","role":"pseudoSymbol","type":"section","abstract":[],"title":"Implement the classification","kind":"section","identifier":"doc:\/\/com.lambdadigamma.mllab\/tutorials\/ML-Lab\/02-setup-implement-model#Implement-the-classification"},"01-import-frameworks.swift":{"syntax":"swift","type":"file","fileName":"ImageClassifier.swift","fileType":"swift","content":["import UIKit","import Vision","import CoreML"],"highlights":[],"identifier":"01-import-frameworks.swift"},"08-chapter2.swift":{"syntax":"swift","type":"file","fileName":"ImageClassifier.swift","fileType":"swift","content":["import UIKit","import Vision","import CoreML","","typealias ImageClassificationHandler = (_ predictions: [ImageClassificationPredication]?) -> Void","","class ImageClassifier {","    ","    static let model = setupModel()","    ","    static func setupModel() -> VNCoreMLModel {}","    ","    \/\/ MARK: - Request Handling -","    ","    private var predictionHandlers = [VNRequest: ImageClassificationHandler]()","    ","    ","}"],"highlights":[{"line":11},{"line":13},{"line":14},{"line":15},{"line":16},{"line":17}],"identifier":"08-chapter2.swift"},"doc://com.lambdadigamma.mllab/tutorials/Tutorial-Table-of-Contents/Add-the-next-chapter-title-here.":{"url":"\/tutorials\/tutorial-table-of-contents\/add-the-next-chapter-title-here.","role":"article","type":"topic","abstract":[],"title":"Add the next chapter title here.","kind":"article","identifier":"doc:\/\/com.lambdadigamma.mllab\/tutorials\/Tutorial-Table-of-Contents\/Add-the-next-chapter-title-here."},"doc://com.lambdadigamma.mllab/tutorials/ML-Lab/01-Setup-the-project#Summary":{"url":"\/tutorials\/ml-lab\/01-setup-the-project#Summary","role":"pseudoSymbol","type":"section","abstract":[{"type":"text","text":"Learn how to add an Core ML file to your app and explore the built-in capabilities of Xcode."}],"title":"Summary","kind":"section","identifier":"doc:\/\/com.lambdadigamma.mllab\/tutorials\/ML-Lab\/01-Setup-the-project#Summary"},"13-chapter2.swift":{"syntax":"swift","type":"file","fileName":"ImageClassifier.swift","fileType":"swift","content":["import UIKit","import Vision","import CoreML","","typealias ImageClassificationHandler = (_ predictions: [ImageClassificationPredication]?) -> Void","","class ImageClassifier {","    ","    static let model = setupModel()","    ","    static func setupModel() -> VNCoreMLModel {}","    ","    \/\/ MARK: - Request Handling -","    ","    private var predictionHandlers = [VNRequest: ImageClassificationHandler]()","    ","    func makePredictions(","        for photo: UIImage,","        completionHandler: @escaping ImageClassificationHandler","    ) throws {","        ","        let orientation = CGImagePropertyOrientation(photo.imageOrientation)","        ","        guard let photoImage = photo.cgImage else {","            fatalError(\"Photo doesn't have underlying CGImage.\")","        }","        ","        let imageClassificationRequest = createImageClassificationRequest()","        ","        predictionHandlers[imageClassificationRequest] = completionHandler","        ","        let handler = VNImageRequestHandler(","            cgImage: photoImage,","            orientation: orientation","        )","        ","        try handler.perform([imageClassificationRequest])","        ","    }","    ","    private func createImageClassificationRequest() -> VNCoreMLRequest {","        ","        let imageClassificationRequest = VNCoreMLRequest(","            model: Self.model,","            completionHandler: visionRequestHandler","        )","        ","        imageClassificationRequest.imageCropAndScaleOption = .centerCrop","        ","        return imageClassificationRequest","    }","    ","    private func visionRequestHandler(_ request: VNRequest, error: Error?) {","        ","        guard let predictionHandler = predictionHandlers.removeValue(","            forKey: request","        ) else {","            fatalError(\"Every request must have a prediction handler.\")","        }","        ","        var predictions: [ImageClassificationPredication]? = nil","        ","        \/\/ Call the client's completion handler after the method returns.","        defer {","            \/\/ Send the predictions back to the client.","            predictionHandler(predictions)","        }","        ","        if let error = error {","            print(\"Vision image classification error: \\(error.localizedDescription)\")","            return","        }","        ","        if request.results == nil {","            print(\"Vision request had no results.\")","            return","        }","        ","        \/\/ ...","        ","    }","    ","}"],"highlights":[{"line":63},{"line":64},{"line":65},{"line":66},{"line":67},{"line":68},{"line":69},{"line":70},{"line":71},{"line":72},{"line":73},{"line":74},{"line":75},{"line":76},{"line":77},{"line":78}],"identifier":"13-chapter2.swift"},"06-chapter2.swift":{"syntax":"swift","type":"file","fileName":"ImageClassifier.swift","fileType":"swift","content":["import UIKit","import Vision","import CoreML","","class ImageClassifier {","    ","    static let model = setupModel()","    ","    static func setupModel() -> VNCoreMLModel {","        ","        let config = MLModelConfiguration()","        config.computeUnits = MLComputeUnits.cpuAndGPU","        ","        let squeezeNet = try? SqueezeNet(","            configuration: config","        )","        ","        guard let squeezeNet = squeezeNet else {","            fatalError(\"App failed to create a model instance.\")","        }","        ","        let underlyingModel = squeezeNet.model","        ","        guard let visionModel = try? VNCoreMLModel(","            for: underlyingModel","        ) else {","            fatalError(\"App failed to create a `VNCoreMLModel` instance.\")","        }","        ","        return visionModel","        ","    }","    ","}"],"highlights":[],"identifier":"06-chapter2.swift"},"03-add-setup-method.swift":{"syntax":"swift","type":"file","fileName":"ImageClassifier.swift","fileType":"swift","content":["import UIKit","import Vision","import CoreML","","class ImageClassifier {","    ","    static let model = setupModel()","    ","    static func setupModel() -> VNCoreMLModel {","        ","        \/\/ todo: implement this","        ","    }","    ","}"],"highlights":[{"line":7},{"line":9},{"line":10},{"line":11},{"line":12},{"line":13}],"identifier":"03-add-setup-method.swift"},"02-create-class.swift":{"syntax":"swift","type":"file","fileName":"ImageClassifier.swift","fileType":"swift","content":["import UIKit","import Vision","import CoreML","","class ImageClassifier {","    ","    ","    ","}"],"highlights":[{"line":4},{"line":5},{"line":6},{"line":7},{"line":8},{"line":9}],"identifier":"02-create-class.swift"},"12-chapter2.swift":{"content":["import UIKit","import Vision","import CoreML","","typealias ImageClassificationHandler = (_ predictions: [ImageClassificationPredication]?) -> Void","","class ImageClassifier {","    ","    static let model = setupModel()","    ","    static func setupModel() -> VNCoreMLModel {}","    ","    \/\/ MARK: - Request Handling -","    ","    private var predictionHandlers = [VNRequest: ImageClassificationHandler]()","    ","    func makePredictions(","        for photo: UIImage,","        completionHandler: @escaping ImageClassificationHandler","    ) throws {","        ","        let orientation = CGImagePropertyOrientation(photo.imageOrientation)","        ","        guard let photoImage = photo.cgImage else {","            fatalError(\"Photo doesn't have underlying CGImage.\")","        }","        ","        let imageClassificationRequest = createImageClassificationRequest()","        ","        predictionHandlers[imageClassificationRequest] = completionHandler","        ","        let handler = VNImageRequestHandler(","            cgImage: photoImage,","            orientation: orientation","        )","        ","        try handler.perform([imageClassificationRequest])","        ","    }","    ","    private func createImageClassificationRequest() -> VNCoreMLRequest {","        ","        let imageClassificationRequest = VNCoreMLRequest(","            model: Self.model,","            completionHandler: visionRequestHandler","        )","        ","        imageClassificationRequest.imageCropAndScaleOption = .centerCrop","        ","        return imageClassificationRequest","    }","    ","    private func visionRequestHandler(_ request: VNRequest, error: Error?) {","        ","        guard let predictionHandler = predictionHandlers.removeValue(","            forKey: request","        ) else {","            fatalError(\"Every request must have a prediction handler.\")","        }","        ","        var predictions: [ImageClassificationPredication]? = nil","        ","        \/\/ ...","        ","    }","    ","}"],"syntax":"swift","fileName":"ImageClassifier.swift","identifier":"12-chapter2.swift","fileType":"swift","type":"file","highlights":[{"line":53},{"line":54},{"line":55},{"line":56},{"line":57},{"line":58},{"line":59},{"line":60},{"line":61},{"line":62},{"line":63},{"line":64},{"line":65},{"line":66}]},"04-chapter2.swift":{"fileType":"swift","highlights":[{"line":11},{"line":12}],"content":["import UIKit","import Vision","import CoreML","","class ImageClassifier {","    ","    static let model = setupModel()","    ","    static func setupModel() -> VNCoreMLModel {","        ","        let config = MLModelConfiguration()","        config.computeUnits = MLComputeUnits.cpuAndGPU","        ","    }","    ","}"],"syntax":"swift","type":"file","identifier":"04-chapter2.swift","fileName":"ImageClassifier.swift"},"05-chapter2.swift":{"highlights":[{"line":14},{"line":15},{"line":16},{"line":17},{"line":18},{"line":19},{"line":20},{"line":21}],"type":"file","fileType":"swift","syntax":"swift","identifier":"05-chapter2.swift","fileName":"ImageClassifier.swift","content":["import UIKit","import Vision","import CoreML","","class ImageClassifier {","    ","    static let model = setupModel()","    ","    static func setupModel() -> VNCoreMLModel {","        ","        let config = MLModelConfiguration()","        config.computeUnits = MLComputeUnits.cpuAndGPU","        ","        let squeezeNet = try? SqueezeNet(","            configuration: config","        )","        ","        guard let squeezeNet = squeezeNet else {","            fatalError(\"App failed to create a model instance.\")","        }","        ","    }","    ","}"]},"doc://com.lambdadigamma.mllab/tutorials/ML-Lab/01-Setup-the-project#Getting-a-model":{"abstract":[{"text":"Learn how to add an Core ML file to your app and explore the built-in capabilities of Xcode.","type":"text"}],"type":"section","url":"\/tutorials\/ml-lab\/01-setup-the-project#Getting-a-model","identifier":"doc:\/\/com.lambdadigamma.mllab\/tutorials\/ML-Lab\/01-Setup-the-project#Getting-a-model","kind":"section","role":"pseudoSymbol","title":"Getting a model"},"SqueezeNet-03.swift":{"highlights":[{"line":37},{"line":38},{"line":39},{"line":40},{"line":41},{"line":42},{"line":43},{"line":44}],"type":"file","fileType":"swift","syntax":"swift","identifier":"SqueezeNet-03.swift","fileName":"SqueezeNet.swift","content":["import CoreML","","","\/\/\/ Model Prediction Input Type","class SqueezeNetInput : MLFeatureProvider {","    ","    \/\/\/ Input image to be classified as color (kCVPixelFormatType_32BGRA) image buffer, 227 pixels wide by 227 pixels high","    var image: CVPixelBuffer","    ","    \/\/ ...","    ","    init(image: CVPixelBuffer) {","        self.image = image","    }","    ","    convenience init(imageWith image: CGImage) throws {","        \/\/ ...","    }","    ","    convenience init(imageAt image: URL) throws {","        \/\/ ...","    }","    ","    func setImage(with image: CGImage) throws {","        \/\/ ...","    }","    ","    func setImage(with image: URL) throws {","        \/\/ ...","    }","    ","}","","\/\/\/ Model Prediction Output Type","class SqueezeNetOutput : MLFeatureProvider {","    ","    \/\/\/ Source provided by CoreML","    private let provider : MLFeatureProvider","    ","    \/\/\/ Probability of each category as dictionary of strings to doubles","    var classLabelProbs: [String : Double] {","        return self.provider.featureValue(for: \"classLabelProbs\")!.dictionaryValue as! [String : Double]","    }","    ","    \/\/ ...","    ","}","","\/\/\/ Class for model loading and prediction","class SqueezeNet {","    ","    \/\/ ...","    ","}"]},"SqueezeNet-04.swift":{"highlights":[{"line":52},{"line":53},{"line":56},{"line":57},{"line":58},{"line":59},{"line":60},{"line":61},{"line":62},{"line":63},{"line":64},{"line":65},{"line":66},{"line":67},{"line":68},{"line":69},{"line":70},{"line":71},{"line":72},{"line":73},{"line":74},{"line":75}],"type":"file","fileType":"swift","syntax":"swift","identifier":"SqueezeNet-04.swift","fileName":"SqueezeNet.swift","content":["import CoreML","","","\/\/\/ Model Prediction Input Type","class SqueezeNetInput : MLFeatureProvider {","    ","    \/\/\/ Input image to be classified as color (kCVPixelFormatType_32BGRA) image buffer, 227 pixels wide by 227 pixels high","    var image: CVPixelBuffer","    ","    \/\/ ...","    ","    init(image: CVPixelBuffer) {","        self.image = image","    }","    ","    convenience init(imageWith image: CGImage) throws {","        \/\/ ...","    }","    ","    convenience init(imageAt image: URL) throws {","        \/\/ ...","    }","    ","    func setImage(with image: CGImage) throws {","        \/\/ ...","    }","    ","    func setImage(with image: URL) throws {","        \/\/ ...","    }","    ","}","","\/\/\/ Model Prediction Output Type","class SqueezeNetOutput : MLFeatureProvider {","    ","    \/\/\/ Source provided by CoreML","    private let provider : MLFeatureProvider","    ","    \/\/\/ Probability of each category as dictionary of strings to doubles","    var classLabelProbs: [String : Double] {","        return self.provider.featureValue(for: \"classLabelProbs\")!.dictionaryValue as! [String : Double]","    }","    ","    \/\/ ...","    ","}","","\/\/\/ Class for model loading and prediction","class SqueezeNet {","    ","    let model: MLModel","    ","    \/\/ ...","    ","    \/\/\/ Construct a model with configuration","    convenience init(configuration: MLModelConfiguration) throws {","        try self.init(contentsOf: type(of:self).urlOfModelInThisBundle, configuration: configuration)","    }","    ","    \/\/ ...","    ","    \/\/\/ Make a prediction using the structured interface","    func prediction(input: SqueezeNetInput) throws -> SqueezeNetOutput {","        return try self.prediction(input: input, options: MLPredictionOptions())","    }","    ","    \/\/\/ Make a prediction using the structured interface","    func prediction(input: SqueezeNetInput, options: MLPredictionOptions) throws -> SqueezeNetOutput {","        let outFeatures = try model.prediction(from: input, options:options)","        return SqueezeNetOutput(features: outFeatures)","    }","    ","    \/\/ ...","    ","}"]},"07-chapter2.swift":{"fileName":"ImageClassifier.swift","type":"file","fileType":"swift","content":["import UIKit","import Vision","import CoreML","","typealias ImageClassificationHandler = (_ predictions: [ImageClassificationPredication]?) -> Void","","class ImageClassifier {","    ","    static let model = setupModel()","    ","    static func setupModel() -> VNCoreMLModel {","        ","        let config = MLModelConfiguration()","        config.computeUnits = MLComputeUnits.cpuAndGPU","        ","        let squeezeNet = try? SqueezeNet(","            configuration: config","        )","        ","        guard let squeezeNet = squeezeNet else {","            fatalError(\"App failed to create a model instance.\")","        }","        ","        let underlyingModel = squeezeNet.model","        ","        guard let visionModel = try? VNCoreMLModel(","            for: underlyingModel","        ) else {","            fatalError(\"App failed to create a `VNCoreMLModel` instance.\")","        }","        ","        return visionModel","        ","    }","    ","}"],"syntax":"swift","identifier":"07-chapter2.swift","highlights":[]}}}