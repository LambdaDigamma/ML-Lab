@Tutorial(time: 30) {
    @Intro(title: "Setup and implement the model") {
        
        
    }
    
    @Section(title: "Setup the model") {
        
        @ContentAndMedia {
            
            In this section we are going to load and instantiate the model.
            
        }
        
        @Steps {
            @Step {
                
                Create a new Swift file named `ImageClassifier.swift`.
                This is going to be the main file where we implement the operation of classifing the image.
                
                @Image(source: "02-01-create-class", alt: "")
            }
            
            @Step {
                
                Start by importing `UIKit`, `Vision` and `CoreML` at the top of the file. We need this frameworks in the course of this tutorial.
                
                @Code(name: "ImageClassifier.swift", file: 01-import-frameworks.swift)
            }
            
            @Step {
            
                Go ahead and create a class in the Swift file.
                
                @Code(name: "ImageClassifier.swift", file: 02-create-class.swift)
            
            }
            
            @Step {
                
                We will start by loading and setting up the model in code.
                Add a static constant `model` and function `setupModel` to the class. We are going to setup the model in the next steps.
                
                > Note: We use a static variable to store a single instance of your model in our app. Instantiating a Core ML model is pretty resource intensive, so we want to instantiate the class once and only once. 
                
                @Code(name: "ImageClassifier.swift", file: 03-add-setup-method.swift)
                
            }
            
            @Step {
                
                First, we have to create a `MLModelConfiguration`.
                This is used to define settings for the way in which a model is executed.
                You can for example specify that the model should use all compute units, CPU and Neural Engine or CPU only.
                
                @Code(name: "ImageClassifier.swift", file: 04-chapter2.swift)
                
            }
            
            @Step {
                
                Now, create an instance of the model wrapper class (e.g. of the class `SqueezeNet` when using the SqueezeNet model).
                This class is autogenerated by Xcode after adding a `.mlmodel` and we are going to look at the content soon. 
                
                We use the `try?` syntax to get an `Optional<SqueezeNet>` back. 
                The variable `modelWrapper` is `nil` if the provided configuration is invalid or the model could not be found in the bundle. 
                
                @Code(name: "ImageClassifier.swift", file: 05-chapter2.swift)
                
            }
            
            @Step {
                
                > Warning:
                As we don't do any runtime changes to the model or load the model from the network, we do not need to use proper error handling here.
                If instantiating the model wrapper fails, we will notice during development and get a `fatalError` – so the App crashes.
                
            }
            
            @Step {
                
                [Bundle](https://developer.apple.com/documentation/foundation/bundle) is a class from the Foundation Framework. It is a class representing your application executable during runtime.
                During compilation of your app, a bundle is being composed. The main application bundle can be accessed using `Bundle.main`.
                
                > As a rule of thumb you can say, that all resources go into your application bundle. For example images, sound files and localization string files can be accessed via the bundle. In our case all relevant model data (our `.mlmodel` file) is being copied to the application bundle.
                
                
                
            }
            
            @Step {
                
                As you might know, Apple's Platforms support a few different Machine Learning tasks like
                Image, Text, Sound, and Activity Classification
                just to name a few.
                
                ---
                
                You might have recognized that all of the given examples have different types of input data. Image Classification needs an image input, Text Classification needs a text, Sound Classification needs a sound and Activity Classification needs raw movement data in a specific format.
                
            }
            
            @Step {
                
                Everything we did by now, is loading and instantiating our model wrapper from the `Bundle`.
                
                You might have wondered what the auto-generated model wrapper does and why it exists. So let's take a quick view at the auto-generated code.
                You can do that by opening your `SqueezeNet.mlmodel` in the Project Navigator and clicking on the label `SqueezeNet` right next to the `Model Class` label.
                
                @Image(source: "02-02-click-model-class", alt: "")
                
                ---
                
                > Warning: Make sure that you compiled your app as the code of the class is generated during compilation. You may have to comment out the existing code in order to compile successfully.
                
            }
            
            
            
            @Step {
                
                You will be greeted by a wall of text. But no worries, we just need to take a quick look at it, as the `Vision` framework will do the heavy lifting for us.
                
            }
            
            @Step {
                
                This file should contain three classes.
                Depending on the name of your model it has a class for input, output and for loading and prediction.
                
                @Code(name: "SqueezeNet.swift", file: 01-SqueezeNet.swift)
                
            }
            
            @Step {
                
                The `SqueezeNetInput` class has an important variable `image` which is the input for the model.
                `CVPixelBuffer` is a very low-level class from `CoreVideo` and stores images in a buffer.
                
                Because working with them is not that straight forward, we get a few so called `convenience` initializer by default. As the name implies a `convenience init` should simplify instantiating an object of this class.
                For example we get options to create a model input from `UIImage` and `URL`.
                
                @Code(name: "SqueezeNet.swift", file: SqueezeNet-02.swift)
                
            }
            
            @Step {
                
                The output class provides a dictionary which maps class labels to their calculated likelihood.
                
                @Code(name: "SqueezeNet.swift", file: SqueezeNet-03.swift)
                
            }
            
            @Step {
                
                The `SqueezeNet` class has exactly the initializer, which we used in the `ImageClassifier` class.
                It also has methods to predict an output from a given output.
                
                --- 
                
                You may have noticed that the input of the model takes only input buffers of size 227x227 pixels.
                
                @Code(name: "SqueezeNet.swift", file: SqueezeNet-04.swift)
                
            }
            
            @Step {
                
                But how do we convert our images to the required format? 
                Thats where the Vision Framework comes into play.
                The Vision Framework provides features to execute common computer vision problems like text recognition on still images and videos.
                It also simplifies dealing with Core ML models so we are going to setup a `VNCoreMLModel` next.
                
            }
            
            @Step {
                
                Go ahead and create a `VNCoreMLModel` from the underlying `MLModel` of the `SqueezeNet` wrapper instance.
                
                
                @Code(name: "ImageClassifier.swift", file: 06-chapter2.swift)
                
            }
            
            @Step {
                
                We now have successfully setup our model and will look at how to use this `VNCoreMLModel` in the next section.
                
            }
            
        }
        
    }
    
    @Section(title: "Implement the classification") {
        
        @ContentAndMedia {
            
            Setup the additional models we need for the `ImageClassifier`.
            
        }
        
        @Steps {
            
            @Step {
                
                Create a new Swift file `ImageClassificationPredication.swift` and add a struct.
                This will store a single result of your classification operation. E.g. label of "elephant" and confidence `0.94`.
                
                @Code(name: "ImageClassificationPredication.swift", file: Section2-01.swift)
                
            }
            
            @Step {
                
                Add a typealias `ImageClassificationHandler` to the top of the file `ImageClassifier.swift`.
                This type will be used as a shorthand for the result closure. It returns an array of `ImageClassificationPredication` and does not exect any return types.
                
                A typealias works like a shorthand. Everywhere you write `ImageClassificationHandler`, it is the same like replacing it with the right side.  
                
                @Code(name: "ImageClassifier.swift", file: 07-chapter2.swift)
                
            }
            
            @Step {
                
                A `VNRequest` is used to execute an image classification operation.
                Each request get's an image input and has a handler which is called with the image classification predictions calculated with the model.
                
                A handler is a closure being passed from the caller to react to the results.
                
            }
            
            @Step {
                
                We store a reference of each request feed into the `ImageClassifier` along with the provided handler.
                
                @Code(name: "ImageClassifier.swift", file: 08-chapter2.swift)
                
            }
            
            @Step {
                
                Create a new file and add an extension to the `CGImagePropertyOrientation`.
                When feeding an image into the Vision framework we need to provide the image orientiation as a `CoreGraphics` image orientation.
                
                We create an initializer which maps the orientation of a `UIImage` (which we can easily handle) to a `CGImagePropertyOrientation`.
                
                @Code(name: "CGImagePropertyOrientation+Extensions.swift", file: CGImagePropertyOrientation+Extensions.swift)
                
            }
            
            @Step {
                
                Add a `makePredictions` function.
                First, obtain the `CGImagePropertyOrientation` and the `CGImage` from the provided `UIImage`.
                
                Next, create an image classification request. We are going to implement the function `createImageClassificationRequest()` in a further step.
                
                
                @Code(name: "ImageClassifier.swift", file: 09-chapter2.swift)
                
            }
            
            @Step {
                
                Store the request in the `predictionHandlers` array with the request as the key and the completion handler passed to the function as the value. This is going to be called when the following request handler finished the classification job.
                
                So create the `VNImageRequestHandler` with a given image and the previously obtained orientation. After that, call the throwing function `perform` on the handler which takes an array of `VNRequest` as parameters.
                
                @Code(name: "ImageClassifier.swift", file: 10-chapter2.swift)
                
            }
            
            @Step {
                
                You can see, that there is an error `Cannot find 'createImageClassificationRequest' in scope`. We are going to fix that now.
                
                Add this method to create a new `VNCoreMLRequest`. It uses the staticly available model which we setup earlier. We use a image crop and scale options of `.centerCrop` – there may be cases where you want a different behaviour, but for our use case this is good.
                
                @Code(name: "ImageClassifier.swift", file: 11-chapter2.swift)
                
                ---
                
                > Warning: Again, you see a warning that `visionRequestHandler` is not available. We will implement this in the next step.
                
            }
            
            @Step {
                
                As the final step, we implement the `visionRequestHandler`.
                This will be the function which get's called every time a `VNCoreMLRequest` succeeds or fails and handles calling the right completion functions.
                
                ---
                
                Based on the request which get's completed, we get the handler function from the `predictionHandlers` dictionary. Next, we need an optional predictions array in which we will store the results produced by the model. 
                
                @Code(name: "ImageClassifier.swift", file: 12-chapter2.swift)
                 
                
            }
            
            @Step {
                
                Next, we use a defer callback to call the retrieved predicition handler with the obtained results.
                
                We also check if the request failed with an error or has no results.
                
                @Code(name: "ImageClassifier.swift", file: 13-chapter2.swift)
                
            }
            
            @Step {
                
                We now cast the request's results as an `VNClassificationObservation` array.
                
                --- 
                > Note: Image classifiers only produce classification observations. However, other Core ML model types can produce other observations. For example, a style transfer model produces `VNPixelBufferObservation` instances.
                
                @Code(name: "ImageClassifier.swift", file: 14-chapter2.swift)
                
            }
            
        }
        
    }
    
    @Section(title: "How to use the image classifier") {
        
        @ContentAndMedia {
            
            Let's look at what we build. You may struggle to understand how data flows here, so we will do a short recap.
            
        }
        
        @Steps {
            
            @Step {
                
                We setup a `VNCoreMLModel` with our image classifier model `SqueezeNet`.
                
            }
            
            @Step {
                
                We as a developer can now create an instance of the `ImageClassifier` class and call `makePredictions` with an image.
                The results are available in the trailing closure.
                
                @Code(name: "Example.swift", file: 01-Example.swift)
                
            }
            
        }
        
    }
    
    
}

